{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7dfb0d-07fb-4f12-852c-a0209e6a81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "이 파일은 custom loss를 실험하기 위해 제작된 파일로 third_train.py 기반으로 진행\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow_addons as tfa\n",
    "from glob import glob\n",
    "from numpy import inf\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data_loader import *\n",
    "from metrics import SELDMetrics, calculate_seld_score\n",
    "from transforms import *\n",
    "from utils import adaptive_clip_grad, apply_kernel_regularizer\n",
    "\n",
    "\n",
    "    \n",
    "class ARGS:\n",
    "    def __init__(self):\n",
    "        self.set('--name', type=str, default='')\n",
    "        self.set('--gpus', type=str, default='3')\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = self.gpus\n",
    "        self.set('--resume', action='store_true')    \n",
    "        self.set('--abspath', type=str, default='/root/datasets')\n",
    "        self.set('--output_path', type=str, default='./output')\n",
    "        self.set('--ans_path', type=str, default='/root/datasets/DCASE2021/metadata_dev/')\n",
    "        self.set('--norm', type=bool, default=True)\n",
    "        self.set('--decay', type=float, default=0.9)\n",
    "        self.set('--sed_th', type=float, default=0.3)\n",
    "        self.set('--lr', type=float, default=0.003)\n",
    "        self.set('--final_lr', type=float, default=0.0001)\n",
    "        self.set('--batch', type=int, default=256)\n",
    "        self.set('--agc', type=bool, default=False)\n",
    "        self.set('--epoch', type=int, default=60)\n",
    "        self.set('--lr_patience', type=int, default=5, help='learning rate decay patience for plateau')\n",
    "        self.set('--patience', type=int, default=100, help='early stop patience')\n",
    "        self.set('--use_acs', type=bool, default=True)\n",
    "        self.set('--use_tfm', type=bool, default=True)\n",
    "        self.set('--use_tdm', action='store_true')\n",
    "        self.set('--schedule', type=bool, default=True)\n",
    "        self.set('--loop_time', type=int, default=5, help='times of train dataset iter for an epoch')\n",
    "        self.set('--lad_doa_thresh', type=int, default=20)\n",
    "        self.set('--nfft', type=int, default=1024)\n",
    "        self.set('--hop', type=int, default=480)\n",
    "        self.set('--len', type=int, default=4)\n",
    "        \n",
    "    def set(self, name, type=str, default=None, action=None, help=''):\n",
    "        if action == 'store_true':\n",
    "            type = bool\n",
    "            default = False\n",
    "        name = name.split('--')[-1]\n",
    "        setattr(self, name, type(default))\n",
    "        \n",
    "args = ARGS()\n",
    "\n",
    "def resnet_block(inp, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "    if stride == 2:\n",
    "        x = tf.keras.layers.AveragePooling2D((2, 2))(inp)\n",
    "    else:\n",
    "        x = inp\n",
    "    x = tf.keras.layers.Conv2D(planes, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Conv2D(planes, (3,3), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    if downsample is not None:\n",
    "        inp = downsample(inp)\n",
    "    x = tf.keras.layers.ReLU()(x + inp)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_layer(inp, planes, blocks, strides=1, dilate=False):\n",
    "    inplanes = inp.shape[-1]\n",
    "    expansion = 1\n",
    "    downsample = None\n",
    "\n",
    "    if strides != 1 or inplanes != planes * expansion:\n",
    "        layers = []\n",
    "        if strides == 2:\n",
    "            layers.append(tf.keras.layers.AveragePooling2D((2, 2)))\n",
    "        layers.append(tf.keras.layers.Conv2D(planes * expansion, kernel_size=1, strides=1, use_bias=False))\n",
    "        layers.append(tf.keras.layers.BatchNormalization())\n",
    "        downsample = tf.keras.Sequential(layers)\n",
    "    \n",
    "    x = resnet_block(inp, planes, strides, downsample)\n",
    "    for _ in range(1, blocks):\n",
    "        x = resnet_block(x, planes)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet(inp, layers, replace_stride_with_dilation=None):\n",
    "    if replace_stride_with_dilation is None:\n",
    "        replace_stride_with_dilation = [False, False, False]\n",
    "    if len(replace_stride_with_dilation) != 3:\n",
    "        raise ValueError(f\"replace_stride_with_dilation should be None or a 3-element tuple, got {replace_stride_with_dilation}\")\n",
    "    x = resnet_layer(inp, 64, layers[0], strides=1)\n",
    "    x = resnet_layer(x, 128, layers[1], strides=2, dilate=replace_stride_with_dilation[0])\n",
    "    x = resnet_layer(x, 256, layers[2], strides=2, dilate=replace_stride_with_dilation[1])\n",
    "    x = resnet_layer(x, 512, layers[3], strides=2, dilate=replace_stride_with_dilation[2])\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block(inp, out_channel, pool_type='avg', pool_size=(2,2)):\n",
    "    x = tf.keras.layers.Conv2D(out_channel, kernel_size=(3,3), padding='same', use_bias=False)(inp)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(out_channel, kernel_size=(3,3), padding='same', use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    if pool_type == 'avg':\n",
    "        x = tf.keras.layers.AveragePooling2D(pool_size)(x)\n",
    "    elif pool_type == 'max':\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size)(x)\n",
    "    elif pool_type == 'avg+max':\n",
    "        x1 = tf.keras.layers.AveragePooling2D(pool_size)(x)\n",
    "        x2 = tf.keras.layers.MaxPooling2D(pool_size)(x)\n",
    "        x = x1 + x2\n",
    "    else:\n",
    "        raise Exception('Wrong pool_type')\n",
    "    return x\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "def get_accdoa_labels(accdoa_in, nb_classes, sed_th=0.3):\n",
    "    x, y, z = accdoa_in[:, :, :nb_classes], accdoa_in[:, :, nb_classes:2*nb_classes], accdoa_in[:, :, 2*nb_classes:]\n",
    "    sed = tf.cast(tf.sqrt(x**2 + y**2 + z**2) > sed_th, tf.float32)\n",
    "    return sed, accdoa_in\n",
    "\n",
    "def generate_trainstep(criterion, config):\n",
    "    # These are statistics from the train dataset\n",
    "    # train_samples = tf.convert_to_tensor(\n",
    "    #     [[58193, 32794, 29801, 21478, 14822, \n",
    "    #     9174, 66527,  6740,  9342,  6498, \n",
    "    #     22218, 49758]],\n",
    "    #     dtype=tf.float32)\n",
    "    # cls_weights = tf.reduce_mean(train_samples) / train_samples\n",
    "    @tf.function\n",
    "    def trainstep(model, x, y, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_p = model(x, training=True)\n",
    "            loss = criterion(y[1], y_p)\n",
    "\n",
    "            # regularizer\n",
    "            # loss += tf.add_n([l.losses[0] for l in model.layers\n",
    "            #                   if len(l.losses) > 0])\n",
    "\n",
    "        grad = tape.gradient(loss, model.trainable_variables)\n",
    "        # apply AGC\n",
    "        if config.agc:\n",
    "            grad = adaptive_clip_grad(model.trainable_variables, grad)\n",
    "        optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
    "\n",
    "        return y_p, loss\n",
    "    return trainstep\n",
    "\n",
    "\n",
    "def generate_teststep(criterion):\n",
    "    @tf.function\n",
    "    def teststep(model, x, y, optimizer=None):\n",
    "        y_p = model(x, training=False)\n",
    "        loss = criterion(y[1], y_p)\n",
    "        return y_p, loss\n",
    "    return teststep\n",
    "\n",
    "\n",
    "def generate_iterloop(criterion, evaluator, writer, \n",
    "                      mode, config=None):\n",
    "    if mode == 'train':\n",
    "        step = generate_trainstep(criterion, config)\n",
    "    else:\n",
    "        step = generate_teststep(criterion)\n",
    "\n",
    "    def iterloop(model, dataset, epoch, optimizer=None):\n",
    "        evaluator.reset_states()\n",
    "        losses = tf.keras.metrics.Mean()\n",
    "\n",
    "        with tqdm(dataset) as pbar:\n",
    "            for x, y in pbar:\n",
    "                preds, loss = step(model, x, y, optimizer)\n",
    "                y, preds = y, get_accdoa_labels(preds, preds.shape[-1] // 3, config.sed_th)\n",
    "                \n",
    "                evaluator.update_states(y, preds)\n",
    "                metric_values = evaluator.result()\n",
    "                seld_score = calculate_seld_score(metric_values)\n",
    "\n",
    "                losses(loss)\n",
    "                if mode == 'train':\n",
    "                    status = OrderedDict({\n",
    "                        'mode': mode,\n",
    "                        'epoch': epoch,\n",
    "                        'lr': optimizer.learning_rate.numpy(),\n",
    "                        'loss': losses.result().numpy(),\n",
    "                        'ER': metric_values[0].numpy(),\n",
    "                        'F': metric_values[1].numpy(),\n",
    "                        'DER': metric_values[2].numpy(),\n",
    "                        'DERF': metric_values[3].numpy(),\n",
    "                        'seldscore': seld_score.numpy()\n",
    "                    })\n",
    "                else:\n",
    "                    status = OrderedDict({\n",
    "                    'mode': mode,\n",
    "                    'epoch': epoch,\n",
    "                    'loss': losses.result().numpy(),\n",
    "                    'ER': metric_values[0].numpy(),\n",
    "                    'F': metric_values[1].numpy(),\n",
    "                    'DER': metric_values[2].numpy(),\n",
    "                    'DERF': metric_values[3].numpy(),\n",
    "                    'seldscore': seld_score.numpy()\n",
    "                    })\n",
    "                pbar.set_postfix(status)\n",
    "\n",
    "        writer.add_scalar(f'{mode}/{mode}_ErrorRate', metric_values[0].numpy(),\n",
    "                          epoch)\n",
    "        writer.add_scalar(f'{mode}/{mode}_F', metric_values[1].numpy(), epoch)\n",
    "        writer.add_scalar(f'{mode}/{mode}_DoaErrorRate', \n",
    "                          metric_values[2].numpy(), epoch)\n",
    "        writer.add_scalar(f'{mode}/{mode}_DoaErrorRateF', \n",
    "                          metric_values[3].numpy(), epoch)\n",
    "        writer.add_scalar(f'{mode}/{mode}_Loss', \n",
    "                          losses.result().numpy(), epoch)\n",
    "        writer.add_scalar(f'{mode}/{mode}_seldScore', \n",
    "                          seld_score.numpy(), epoch)\n",
    "\n",
    "        return seld_score.numpy()\n",
    "    return iterloop\n",
    "\n",
    "\n",
    "def random_ups_and_downs(x, y):\n",
    "    stddev = 0.25\n",
    "    offsets = tf.linspace(tf.random.normal([], stddev=stddev),\n",
    "                          tf.random.normal([], stddev=stddev),\n",
    "                          x.shape[-3])\n",
    "    offsets_shape = [1] * len(x.shape)\n",
    "    offsets_shape[-3] = offsets.shape[0]\n",
    "    offsets = tf.reshape(offsets, offsets_shape)\n",
    "    x = tf.concat([x[..., :4] + offsets, x[..., 4:]], -1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7c06e6-c179-47d2-a463-95bdddc64b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 11:20:49.862210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-18 11:20:51.027299: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38428 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0\n",
      "2021-11-18 11:21:41.068615: W tensorflow/core/common_runtime/bfc_allocator.cc:457] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.59GiB (rounded to 4924800000)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2021-11-18 11:21:41.068664: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] BFCAllocator dump for GPU_0_bfc\n",
      "2021-11-18 11:21:41.068682: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (256): \tTotal Chunks: 5, Chunks in use: 5. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 33B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068694: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068707: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068718: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068731: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068743: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068756: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068768: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068781: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068793: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068805: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068818: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068828: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068839: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068850: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068860: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068871: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068884: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068895: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068906: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068920: I tensorflow/core/common_runtime/bfc_allocator.cc:1011] Bin (268435456): \tTotal Chunks: 3, Chunks in use: 2. 37.53GiB allocated for chunks. 36.69GiB in use in bin. 36.69GiB client-requested in use in bin.\n",
      "2021-11-18 11:21:41.068933: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Bin for 4.59GiB was 256.00MiB, Chunk State: \n",
      "2021-11-18 11:21:41.068952: I tensorflow/core/common_runtime/bfc_allocator.cc:1033]   Size: 854.94MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 256B | Requested Size: 8B | in_use: 1 | bin_num: -1\n",
      "2021-11-18 11:21:41.068961: I tensorflow/core/common_runtime/bfc_allocator.cc:1040] Next region of size 40294875136\n",
      "2021-11-18 11:21:41.068974: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fb786000000 of size 19699200000 next 1\n",
      "2021-11-18 11:21:41.068983: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fbc1c29f000 of size 1280 next 2\n",
      "2021-11-18 11:21:41.068993: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fbc1c29f500 of size 19699200000 next 3\n",
      "2021-11-18 11:21:41.069002: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc0b253e500 of size 256 next 4\n",
      "2021-11-18 11:21:41.069011: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc0b253e600 of size 256 next 5\n",
      "2021-11-18 11:21:41.069020: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc0b253e700 of size 256 next 6\n",
      "2021-11-18 11:21:41.069029: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc0b253e800 of size 256 next 7\n",
      "2021-11-18 11:21:41.069038: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] InUse at 7fc0b253e900 of size 256 next 8\n",
      "2021-11-18 11:21:41.069047: I tensorflow/core/common_runtime/bfc_allocator.cc:1060] Free  at 7fc0b253ea00 of size 896472576 next 18446744073709551615\n",
      "2021-11-18 11:21:41.069055: I tensorflow/core/common_runtime/bfc_allocator.cc:1065]      Summary of in-use Chunks by size: \n",
      "2021-11-18 11:21:41.069067: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 5 Chunks of size 256 totalling 1.2KiB\n",
      "2021-11-18 11:21:41.069077: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2021-11-18 11:21:41.069088: I tensorflow/core/common_runtime/bfc_allocator.cc:1068] 2 Chunks of size 19699200000 totalling 36.69GiB\n",
      "2021-11-18 11:21:41.069098: I tensorflow/core/common_runtime/bfc_allocator.cc:1072] Sum Total of in-use chunks: 36.69GiB\n",
      "2021-11-18 11:21:41.069107: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] total_region_allocated_bytes_: 40294875136 memory_limit_: 40294875136 available bytes: 0 curr_region_allocation_bytes_: 80589750272\n",
      "2021-11-18 11:21:41.069122: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] Stats: \n",
      "Limit:                     40294875136\n",
      "InUse:                     39398402560\n",
      "MaxInUse:                  39398402560\n",
      "NumAllocs:                           8\n",
      "MaxAllocSize:              19699200000\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2021-11-18 11:21:41.069133: W tensorflow/core/common_runtime/bfc_allocator.cc:468] **************************************************************************************************__\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9592/1076054698.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;31m# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m# input_shape = [320, 128, 7]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9592/1076054698.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# data load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0mvalset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9592/1076054698.py\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(config, mode)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseconds\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mbatch_transforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msplit_total_labels_to_sed_doa\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    683\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \"\"\"\n\u001b[0;32m--> 685\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3842\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3843\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3844\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3845\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3846\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    127\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 129\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    130\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[0;32m--> 271\u001b[0;31m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[1;32m    272\u001b[0m                         allow_broadcast=True)\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data. Its structure depends on your model and\n",
    "        # on what you pass to `fit()`.\n",
    "        x, y = data\n",
    "        if not isinstance(y, tuple):\n",
    "            y = (y,)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)  # Forward pass\n",
    "            if not isinstance(y_pred, (tuple, list)):\n",
    "                y_pred = (y_pred,)\n",
    "            # Compute the loss value\n",
    "            # (the loss function is configured in `compile()`)\n",
    "            loss = self.compiled_loss(y, y_pred)\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        \n",
    "        self.compiled_metrics.update_state(y, y_pred[0])\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "\n",
    "# https://github.com/qiuqiangkong/audioset_tagging_cnn/tree/master/pytorch\n",
    "def get_model(input_shape):\n",
    "    inp = tf.keras.layers.Input(shape = input_shape)\n",
    "    class_num = 1\n",
    "    \n",
    "    x = inp\n",
    "    conv1 = Convolution2D(96, kernel_size=(1, 7), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='conv1')(x)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(96, kernel_size=(7, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='conv2')(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='conv3')(conv2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(2, 1), name='conv4')(conv3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(4, 1), name='conv5')(conv4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "\n",
    "    conv6 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(8, 1), name='conv6')(conv5)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "\n",
    "    conv7 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(16, 1), name='conv7')(conv6)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "\n",
    "    conv8 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(32, 1), name='conv8')(conv7)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "\n",
    "    conv9 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='conv9')(conv8)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(2, 2), name='conv10')(conv9)\n",
    "    conv10 = BatchNormalization()(conv10)\n",
    "    conv10 = Activation('relu')(conv10)\n",
    "\n",
    "    conv11 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(4, 4), name='conv11')(conv10)\n",
    "    conv11 = BatchNormalization()(conv11)\n",
    "    conv11 = Activation('relu')(conv11)\n",
    "\n",
    "    conv12 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(8, 8), name='conv12')(conv11)\n",
    "    conv12 = BatchNormalization()(conv12)\n",
    "    conv12 = Activation('relu')(conv12)\n",
    "\n",
    "    conv13 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(16, 16), name='conv13')(conv12)\n",
    "    conv13 = BatchNormalization()(conv13)\n",
    "    conv13 = Activation('relu')(conv13)\n",
    "\n",
    "    conv14 = Convolution2D(96, kernel_size=(5, 5), strides=(1, 1), padding='same', dilation_rate=(32, 32), name='conv14')(conv13)\n",
    "    conv14 = BatchNormalization()(conv14)\n",
    "    conv14 = Activation('relu')(conv14)\n",
    "\n",
    "    conv15 = Convolution2D(8, kernel_size=(1, 1), strides=(1, 1), padding='same', dilation_rate=(1, 1), name='conv15')(conv14)\n",
    "    conv15 = BatchNormalization()(conv15)\n",
    "    conv15 = Activation('relu')(conv15)\n",
    "\n",
    "    AVfusion = TimeDistributed(Flatten())(conv15)\n",
    "\n",
    "    lstm = Bidirectional(LSTM(400, return_sequences=True),merge_mode='sum')(AVfusion)\n",
    "\n",
    "    fc1 = Dense(600, name=\"fc1\", activation='relu', kernel_initializer=tf.keras.initializers.HeNormal(seed=27))(lstm)\n",
    "    fc2 = Dense(600, name=\"fc2\", activation='relu', kernel_initializer=tf.keras.initializers.HeNormal(seed=42))(fc1)\n",
    "    fc3 = Dense(600, name=\"fc3\", activation='relu', kernel_initializer=tf.keras.initializers.HeNormal(seed=65))(fc2)\n",
    "\n",
    "    complex_mask = Dense(inp.shape[-2] * inp.shape[-1] * class_num, name=\"complex_mask\", kernel_initializer=tf.keras.initializers.GlorotUniform(seed=87))(fc3)\n",
    "\n",
    "    complex_mask_out = Reshape((inp.shape[-3], inp.shape[-2], -1))(complex_mask)\n",
    "    return CustomModel(inputs=inp, outputs=complex_mask_out)\n",
    "\n",
    "\n",
    "def get_dataset(config, mode: str = 'train'):\n",
    "    path = os.path.join(config.abspath, 'DCASE2021')\n",
    "    name = os.path.join(path, f'foa_dev_{mode}_stft_{config.nfft}_{config.hop}')\n",
    "    seconds = config.len\n",
    "    with ThreadPoolExecutor() as pool:\n",
    "        x = list(pool.map(lambda x: joblib.load(x), sorted(glob(name + '/*.joblib'))))\n",
    "    x = np.stack(x, 0).transpose(0,2,3,1)\n",
    "    y = joblib.load(os.path.join(path, f'foa_dev_{mode}_label.joblib'))\n",
    "    resolution = x.shape[1] // y.shape[1]\n",
    "\n",
    "    x = x.reshape([-1, seconds * 10 * resolution] + [*x.shape[2:]])\n",
    "    x = np.concatenate([x.real, x.imag], -1)\n",
    "    y = y.reshape([-1, seconds * 10, y.shape[2]])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    batch_transforms = [split_total_labels_to_sed_doa]\n",
    "\n",
    "    frame_num = 30\n",
    "    dataset = dataset.batch(config.batch, drop_remainder=False)\n",
    "\n",
    "    if mode == 'train':\n",
    "        dataset.shuffle(x.shape[0])\n",
    "    for transforms in batch_transforms:\n",
    "        dataset = apply_ops(dataset, transforms)\n",
    "        \n",
    "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "def main(config):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = config.gpus\n",
    "    n_classes = 12\n",
    "    name = '_'.join(['2', str(config.lr), str(config.final_lr)])\n",
    "    if config.schedule:\n",
    "        name += '_schedule'\n",
    "    if config.norm:\n",
    "        name += '_norm'\n",
    "    config.name = name + '_' + config.name\n",
    "\n",
    "    # data load\n",
    "    trainset = get_dataset(config, 'train')\n",
    "    valset = get_dataset(config, 'val')\n",
    "    testset = get_dataset(config, 'test')\n",
    "\n",
    "    tensorboard_path = os.path.join('./tensorboard_log', config.name)\n",
    "    if not os.path.exists(tensorboard_path):\n",
    "        print(f'tensorboard log directory: {tensorboard_path}')\n",
    "        os.makedirs(tensorboard_path)\n",
    "    writer = SummaryWriter(logdir=tensorboard_path)\n",
    "\n",
    "    model_path = os.path.join('./saved_model', config.name)\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f'saved model directory: {model_path}')\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    x, _ = [(x, y) for x, y in valset.take(1)][0]\n",
    "    input_shape = x.shape[1:]\n",
    "    model = get_model(input_shape)\n",
    "    kernel_regularizer = tf.keras.regularizers.l1_l2(l1=0, l2=0.0001)\n",
    "    model = apply_kernel_regularizer(model, kernel_regularizer)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(config.lr)\n",
    "    criterion = tf.keras.losses.MSE\n",
    "\n",
    "\n",
    "    if config.resume:\n",
    "        _model_path = sorted(glob(model_path + '/*.hdf5'))\n",
    "        if len(_model_path) == 0:\n",
    "            raise ValueError('the model does not exist, cannot be resumed')\n",
    "        model = tf.keras.models.load_model(_model_path[0])\n",
    "\n",
    "    best_score = inf\n",
    "    evaluator = SELDMetrics(\n",
    "        doa_threshold=config.lad_doa_thresh, n_classes=n_classes, sed_th=config.sed_th)\n",
    "\n",
    "    train_iterloop = generate_iterloop(\n",
    "        criterion, evaluator, writer, 'train', config=config)\n",
    "    val_iterloop = generate_iterloop(\n",
    "        criterion, evaluator, writer, 'val', config=config)\n",
    "    test_iterloop = generate_iterloop(\n",
    "        criterion, evaluator, writer, 'test', config=config)\n",
    "\n",
    "    lr_decay_patience = 0\n",
    "    for epoch in range(config.epoch):\n",
    "\n",
    "        # train loop\n",
    "        train_iterloop(model, trainset, epoch, optimizer)\n",
    "        score = val_iterloop(model, valset, epoch)\n",
    "        test_iterloop(model, testset, epoch)\n",
    "\n",
    "        if best_score > score:\n",
    "            os.system(f'rm -rf {model_path}/bestscore_{best_score}.hdf5')\n",
    "            best_score = score\n",
    "            tf.keras.models.save_model(\n",
    "                model, \n",
    "                os.path.join(model_path, f'bestscore_{best_score}.hdf5'), \n",
    "                include_optimizer=False)\n",
    "            lr_decay_patience = 0\n",
    "        else:\n",
    "            if not config.schedule:\n",
    "                lr_decay_patience += 1\n",
    "                print(f'lr_decay_patience: {lr_decay_patience}')\n",
    "            if lr_decay_patience >= config.lr_patience and config.decay != 1:\n",
    "                print(f'lr: {optimizer.learning_rate.numpy():.3} -> {(optimizer.learning_rate * config.decay).numpy():.3}')\n",
    "                optimizer.learning_rate = optimizer.learning_rate * config.decay\n",
    "                lr_decay_patience = 0\n",
    "        if config.schedule:\n",
    "            # decay_coefficient = (config.final_lr / config.lr) ** (1 / config.epoch)\n",
    "            # print(f'lr: {optimizer.learning_rate.numpy():.3} -> {(optimizer.learning_rate * decay_coefficient).numpy():.3}')\n",
    "            # optimizer.learning_rate = optimizer.learning_rate * decay_coefficient\n",
    "            decay_coefficient = (config.final_lr - config.lr) / config.epoch\n",
    "            print(f'lr: {optimizer.learning_rate.numpy():.3} -> {(optimizer.learning_rate + decay_coefficient).numpy():.3}')\n",
    "            optimizer.learning_rate = optimizer.learning_rate + decay_coefficient\n",
    "\n",
    "            \n",
    "\n",
    "    # end of training\n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main(args)\n",
    "    # os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "    # input_shape = [320, 128, 7]\n",
    "    # model = get_model(input_shape)\n",
    "    # model.summary()\n",
    "    # from model_flop import get_flops\n",
    "    # print(get_flops(model))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e395ec25-8bb3-4bcc-a1be-4b0acbe0a00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [02:07<00:00,  3.14it/s]\n",
      "100%|██████████| 100/100 [00:32<00:00,  3.11it/s]\n",
      "100%|██████████| 100/100 [00:31<00:00,  3.22it/s]\n"
     ]
    }
   ],
   "source": [
    "def open_wav(path, sr = False):\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, sampling_rate = tf.audio.decode_wav(audio)\n",
    "    if sr:\n",
    "        return sampling_rate\n",
    "    return audio.numpy()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_wav_dataset(config, mode: str = 'train'):\n",
    "    path = os.path.join(config.abspath, f'DCASE2021/foa_dev/dev-{mode}')\n",
    "    save_path = os.path.join(config.abspath, f'DCASE2021/foa_dev_{mode}_stft_1024_480')\n",
    "    y = joblib.load(os.path.join(config.abspath, 'DCASE2021', f'foa_dev_{mode}_label.joblib'))\n",
    "    sr = open_wav(sorted(glob(path + '/*.wav'))[0], sr=True)\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "        \n",
    "    for i in tqdm(sorted(glob(path + '/*.wav'))):\n",
    "        wav = tf.transpose(open_wav(i), (1,0))\n",
    "        stft = tf.signal.stft(wav, 1024, 480, 1024, pad_end=True)\n",
    "        stft = tf.concat([stft.numpy().real, stft.numpy().imag], 0)\n",
    "        joblib.dump(stft.numpy(), os.path.join(save_path, os.path.splitext(os.path.basename(i))[0] + '.joblib'))\n",
    "get_wav_dataset(args)\n",
    "get_wav_dataset(args, 'val')\n",
    "get_wav_dataset(args, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
